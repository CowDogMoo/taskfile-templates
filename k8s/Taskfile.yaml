---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

set: [pipefail]

includes:
  helm:
    taskfile: ./helm
  argo:
    taskfile: ./gitops/argo
    optional: true
  flux:
    taskfile: ./gitops/flux
    optional: true

tasks:
  # ============================================
  # Prerequisites & Validation
  # ============================================
  check-kind:
    desc: Validate that kind is installed
    cmds:
      - |
        if ! command -v kind &> /dev/null; then
          echo -e "Error: 'kind' command not found"
          echo "Please install kind: https://kind.sigs.k8s.io/docs/user/quick-start/#installation"
          exit 1
        fi
        echo "✓ kind is installed ($(kind version))"
    silent: true

  check-kubectl:
    desc: Validate that kubectl is installed
    cmds:
      - |
        if ! command -v kubectl &> /dev/null; then
          echo -e "Error: 'kubectl' command not found"
          echo "Please install kubectl: https://kubernetes.io/docs/tasks/tools/"
          exit 1
        fi
        echo "✓ kubectl is installed ($(kubectl version --client -o json | jq -r .clientVersion.gitVersion))"
    silent: true

  # ============================================
  # Cluster Management
  # ============================================
  create-kind:
    deps: [check-kind]
    desc: Create a kind cluster with one control plane and one worker node
    vars:
      CLUSTER_NAME: '{{ .CLUSTER_NAME | default "test-cluster" }}'
      CONFIG_PATH: '{{ .CONFIG_PATH | default "/tmp/kind-config-${CLUSTER_NAME}.yaml" }}'
    cmds:
    - |
        # Create config directory if it doesn't exist
        mkdir -p "$(dirname {{ .CONFIG_PATH }})"

        # Generate kind configuration
        cat > {{ .CONFIG_PATH }} <<EOF
        kind: Cluster
        apiVersion: kind.x-k8s.io/v1alpha4
        nodes:
        - role: control-plane
        - role: worker
        EOF

        # Create cluster
        kind create cluster --name {{ .CLUSTER_NAME }} --config {{ .CONFIG_PATH }}

        # Cleanup config file if using default temp location
        if [[ "{{ .CONFIG_PATH }}" == "/tmp/kind-config-${CLUSTER_NAME}.yaml" ]]; then
          rm -f {{ .CONFIG_PATH }}
        fi
    silent: true

  destroy-kind:
    deps: [check-kind]
    desc: Delete the kind cluster and cleanup related resources
    vars:
      CLUSTER_NAME: '{{ .CLUSTER_NAME | default "test-cluster" }}'
      CONFIG_PATH: '{{ .CONFIG_PATH | default "/tmp/kind-config-${CLUSTER_NAME}.yaml" }}'
    cmds:
      - |
        # Check if the cluster exists
        if kind get clusters | grep -q "^{{ .CLUSTER_NAME }}$"; then
          echo "Deleting kind cluster: {{ .CLUSTER_NAME }}"
          kind delete cluster --name {{ .CLUSTER_NAME }}

          # Cleanup config file if using default temp location
          if [[ "{{ .CONFIG_PATH }}" == "/tmp/kind-config-${CLUSTER_NAME}.yaml" ]]; then
            rm -f {{ .CONFIG_PATH }}
          fi
        else
          echo "No cluster named '{{ .CLUSTER_NAME }}' found"
          exit 0
        fi

        # Additional cleanup for Docker resources
        echo "Cleaning up any remaining Docker resources..."
        docker container prune -f
        docker network prune -f
    silent: true

  # ============================================
  # Node Operations
  # ============================================
  describe-node:
    deps: [check-kubectl]
    desc: Describe a specific node
    summary: |
      Get detailed information about a specific node

      Example:
        task k8s:describe-node NODE=k8s1
    vars:
      NODE: "{{.NODE}}"
    cmds:
      - |
        if [ -z "{{.NODE}}" ]; then
          echo -e "Error: NODE variable not set"
          echo "Usage: task k8s:describe-node NODE=nodename"
          exit 1
        fi
        kubectl describe node {{.NODE}}
    silent: true

  drain-node:
    deps: [check-kubectl]
    desc: Drain a node for maintenance
    summary: |
      Safely evict all pods from a node for maintenance

      Example:
        task k8s:drain-node NODE=k8s1
    vars:
      NODE: "{{.NODE}}"
    prompt: "This will drain node {{.NODE}}. Continue?"
    cmds:
      - |
        if [ -z "{{.NODE}}" ]; then
          echo -e "Error: NODE variable not set"
          echo "Usage: task k8s:drain-node NODE=nodename"
          exit 1
        fi
        kubectl drain {{.NODE}} --ignore-daemonsets --delete-emptydir-data
    silent: true

  list-node-pods:
    deps: [check-kubectl]
    desc: List all pods running on a specific node across all namespaces
    vars:
      NODE_NAME: '{{ .NODE_NAME | default "node1" }}'
    cmds:
      - kubectl get pods --all-namespaces --field-selector spec.nodeName={{ .NODE_NAME }} -o wide
    silent: true

  uncordon-node:
    deps: [check-kubectl]
    desc: Uncordon a node after maintenance
    summary: |
      Mark a node as schedulable again after maintenance

      Example:
        task k8s:uncordon-node NODE=k8s1
    vars:
      NODE: "{{.NODE}}"
    cmds:
      - |
        if [ -z "{{.NODE}}" ]; then
          echo -e "Error: NODE variable not set"
          echo "Usage: task k8s:uncordon-node NODE=nodename"
          exit 1
        fi
        kubectl uncordon {{.NODE}}
    silent: true

  # ============================================
  # Troubleshooting & Debugging
  # ============================================
  delete-error-pods:
    deps: [check-kubectl]
    desc: Delete all pods with CreateContainerError, ContainerCreating, or CrashLoopBackOff status
    summary: |
      Find and delete all pods across all namespaces that are in error states:
      - CreateContainerError
      - ContainerCreating (stuck)
      - CrashLoopBackOff
      - Terminating (stuck)

      Example:
        task k8s:delete-error-pods
    cmds:
      - |
        set +e  # Don't exit on error
        echo "Searching for pods in error states..."

        # Get all pods and their status
        TEMP_FILE=$(mktemp)
        kubectl get pods -A --no-headers -o wide > "$TEMP_FILE" 2>/dev/null

        # Filter for error states
        ERROR_PODS=$(cat "$TEMP_FILE" | \
          grep -E "CreateContainerError|ContainerCreating|CrashLoopBackOff|Terminating" | \
          awk '{print $1" "$2}' || true)

        rm -f "$TEMP_FILE"

        if [ -z "$ERROR_PODS" ]; then
          echo "No pods found in error states"
          exit 0
        fi

        # Count the pods to be deleted
        POD_COUNT=$(echo "$ERROR_PODS" | wc -l | tr -d ' ')
        echo "Found $POD_COUNT pod(s) in error states:"
        echo "$ERROR_PODS" | while read ns pod; do
          [ -n "$ns" ] && [ -n "$pod" ] && echo "  - $ns/$pod"
        done
        echo ""

        # Delete each pod
        SUCCESS_COUNT=0
        FAIL_COUNT=0
        echo "$ERROR_PODS" | while read ns pod; do
          if [ -n "$ns" ] && [ -n "$pod" ]; then
            echo -n "Deleting pod $pod in namespace $ns... "
            if kubectl delete pod -n "$ns" "$pod" --grace-period=0 --force >/dev/null 2>&1; then
              echo "✓"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
            else
              if kubectl get pod -n "$ns" "$pod" >/dev/null 2>&1; then
                echo "✗ (failed to delete)"
                FAIL_COUNT=$((FAIL_COUNT + 1))
              else
                echo "✓ (already deleted)"
                SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              fi
            fi
          fi
        done

        echo ""
        echo "Completed: Successfully handled $SUCCESS_COUNT pod(s)"
        [ $FAIL_COUNT -gt 0 ] && echo "Warning: Failed to delete $FAIL_COUNT pod(s)"
        exit 0
    silent: true

  destroy-stuck-ns:
    deps: [check-kubectl]
    desc: Remove finalizers from stuck Kubernetes namespaces
    vars:
      PROXY_PORT: '{{ .PROXY_PORT | default "8001" }}'
    cmds:
      - |
        # Find all namespaces in Terminating state
        TERMINATING_NS=$(kubectl get ns --field-selector status.phase=Terminating -o jsonpath='{.items[*].metadata.name}')
        if [ -z "$TERMINATING_NS" ]; then
            echo "No stuck namespaces found"
            exit 0
        fi
        echo "Found stuck namespaces: $TERMINATING_NS"

        cleanup_proxy() {
            local proxy_pid=$1
            local proxy_port=$2
            if [ ! -z "$proxy_pid" ]; then
                # Kill the proxy and any child processes
                pkill -P $proxy_pid 2>/dev/null || true
                kill -9 $proxy_pid 2>/dev/null || true
                # Ensure no hanging processes
                if command -v lsof >/dev/null 2>&1; then
                    lsof -ti :$proxy_port | xargs -r kill -9 2>/dev/null || true
                else
                    fuser -k $proxy_port/tcp 2>/dev/null || true
                fi
            fi
        }

        # Kill any existing proxy on the port
        if command -v lsof >/dev/null 2>&1; then
            lsof -ti :{{ .PROXY_PORT }} | xargs -r kill -9 2>/dev/null || true
        else
            fuser -k {{ .PROXY_PORT }}/tcp 2>/dev/null || true
        fi

        # Process each namespace sequentially
        for ns in $TERMINATING_NS; do
            echo "Removing finalizers from namespace: $ns"

            # Retry loop to handle conflicts (409 errors)
            for attempt in {1..5}; do
                # Get the latest namespace definition
                if ! kubectl get namespace $ns -o json > "tmp_$ns.json"; then
                    echo "Failed to fetch namespace $ns, skipping."
                    break
                fi

                # Remove finalizers if they exist
                jq '.spec.finalizers = []' "tmp_$ns.json" > "tmp_clean_$ns.json"

                # Start proxy with cleanup
                kubectl proxy --port={{ .PROXY_PORT }} &
                PROXY_PID=$!

                # Wait for proxy to be ready
                for i in $(seq 1 10); do
                    if curl -s http://127.0.0.1:{{ .PROXY_PORT }} >/dev/null; then
                        break
                    fi
                    if [ $i -eq 10 ]; then
                        echo "Proxy failed to start for namespace: $ns"
                        cleanup_proxy $PROXY_PID {{ .PROXY_PORT }}
                        exit 1
                    fi
                    sleep 1
                done

                # Attempt to update the namespace
                RESPONSE=$(curl -s -w "%{http_code}" -o /dev/null -k -H "Content-Type: application/json" -X PUT --data-binary @"tmp_clean_$ns.json" \
                    http://127.0.0.1:{{ .PROXY_PORT }}/api/v1/namespaces/$ns/finalize)

                cleanup_proxy $PROXY_PID {{ .PROXY_PORT }}

                if [[ "$RESPONSE" == "200" ]]; then
                    echo "Successfully updated namespace: $ns"
                    break
                elif [[ "$RESPONSE" == "409" ]]; then
                    echo "Conflict detected for $ns (attempt $attempt). Retrying..."
                    sleep 2
                else
                    echo "Failed to update namespace: $ns with HTTP response $RESPONSE"
                    exit 1
                fi
            done

            rm -f "tmp_$ns.json" "tmp_clean_$ns.json"
            echo "Completed processing namespace: $ns"
        done

        echo "All stuck namespaces have been processed"
    silent: true

  get-events:
    deps: [check-kubectl]
    desc: Get cluster events sorted by timestamp
    vars:
      NAMESPACE: '{{.NAMESPACE | default ""}}'
    cmds:
      - |
        if [ -n "{{.NAMESPACE}}" ]; then
          kubectl get events -n {{.NAMESPACE}} --sort-by='.lastTimestamp'
        else
          kubectl get events --all-namespaces --sort-by='.lastTimestamp'
        fi
    silent: true

  get-pods-all-ns:
    deps: [check-kubectl]
    desc: Get all pods across all namespaces
    cmds:
      - kubectl get pods --all-namespaces -o wide
    silent: true
